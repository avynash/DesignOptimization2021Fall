{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Homework 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avynash/DesignOptimization2021Fall/blob/main/Homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "touched-logic"
      },
      "source": [
        "# Theory/Computation Problems\n",
        "\n",
        "### Problem 1 (20 points) \n",
        "Show that the stationary point (zero gradient) of the function\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
        "\\end{aligned}\n",
        "$$\n",
        "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n",
        "\n",
        "### Problem 2 (50 points) \n",
        "\n",
        "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
        "\n",
        "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n",
        "\n",
        "### Problem 3 (10 points) \n",
        "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
        "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
        "* (5 points) In what conditions will $f(g(x))$ be convex?\n",
        "\n",
        "### Problem 4 (bonus 10 points)\n",
        "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
        "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
      ],
      "id": "touched-logic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "collected-carbon"
      },
      "source": [
        "# Design Problems\n",
        "\n",
        "### Problem 5 (20 points) \n",
        "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
        "\n",
        "* (5 points) Formulate this problem as an optimization problem. \n",
        "* (5 points) Is your problem convex?\n",
        "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
        "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
      ],
      "id": "collected-carbon"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moderate-twins"
      },
      "source": [
        "# Note\n",
        "\n",
        "For this homework, you may want to attach sketches as means to explain your ideas. Here is how you can attach images.\n",
        "\n",
        "![everly1](img/everly7.jpg)"
      ],
      "id": "moderate-twins"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJZrT6GQBAjS"
      },
      "source": [
        "###Problem 1:\n",
        "Given function: $$ f(x) = f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2 $$ \n",
        "\n",
        "Gradient of the function g = \\begin{bmatrix} \n",
        "\tdf/dx_1  \\\\\n",
        "\tdf/dx_2  \\\\\n",
        "\t\\end{bmatrix} \n",
        "\n",
        "  = \\begin{bmatrix} \n",
        "\t4x_1 - 4x_2  \\\\\n",
        "\t-4x_1 + 3x_2 + 1  \\\\\n",
        "\t\\end{bmatrix} \n",
        "\n",
        "And now the Hessian, H = \\begin{bmatrix} \n",
        "\td^2f/dx_1^2 &  d^2f/dx_1.dx_2  \\\\\n",
        "\td^2f/dx_1.dx_2 & d^2f/dx_2^2 \\\\\n",
        "\t\\end{bmatrix}\n",
        " \n",
        " = \\begin{bmatrix} \n",
        "\t4 & -4 \\\\\n",
        "\t-4 & 3  \\\\\n",
        "\t\\end{bmatrix}\n",
        "\n",
        "  The eigen values of the hessian are: $$ (7+ \\sqrt(65)) /2  > 0 $$\n",
        "   $$ (7- \\sqrt(65)) /2  < 0 $$\n",
        "\n",
        "   So, we have the saddle point at f where the gradient should be 0\n",
        "\n",
        "   $$ 4x_1 - 4x_2 = 0\n",
        "   \\implies x_1 = x_2$$\n",
        "\n",
        "   $$ -4x_1 + 3x_2 +1 = 0\n",
        "   \\implies x_1 = 1$$\n",
        "\n",
        "   So the saddle point is (1,1)\n",
        "   \n",
        "   $$ f(x) = f(x_0) + g_x^T(x^T - x_0) +  1/2(x - x_0)^T H_x ( x-x_0)  $$\n",
        "\n",
        "   $$ = f(x_0) + 1/2(x-x_0)H_x ( x-x_0)   $$\n",
        "\n",
        "   $$ \\implies f(x) - f(x_0) = 1/2(x-x_0)H_x ( x-x_0)  $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "eJZrT6GQBAjS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPyPQeBsFnG9"
      },
      "source": [
        "###Problem 2:\n",
        "\n",
        "Given plane: $x_1+2x_2+3x_3=1$\n",
        "\n",
        "we can give it as $x_1 = 1 -2x_2+3x_3$\n",
        "\n",
        "The distance between the given points $(-1, 0, 1)^T$ and the plane is : \n",
        "\n",
        "$$ f = (x_1 + 1)^2 + x_2^2 + (x_3 - 1)^2 $$\n",
        "\n",
        "Now, $$   f = (-2x_2 - 3x_3 +2)^2 + x_2^2 + (x_3 -1 )^2           $$\n",
        "\n",
        "which is $$ 5x_2^2 + 10x_3^2 - 8x_2 -14x_3 + 12x_2 x_3 + 5   $$\n",
        "\n",
        "\n",
        "Gradient g = \\begin{bmatrix} \n",
        "\tdf/dx_1  \\\\\n",
        "\tdf/dx_2  \\\\\n",
        "\t\\end{bmatrix} \n",
        "\n",
        "  = \\begin{bmatrix} \n",
        "\t0  \\\\\n",
        "\t0  \\\\\n",
        "\t\\end{bmatrix} \n",
        "\n",
        "  $$ \\implies 10x_2 + 12x_3 -8 = 0    $$\n",
        "\n",
        "  $$ \\implies 6x_2 + 10x_3 -7 = 0   $$\n",
        "\n",
        "  We now have: \n",
        "   $$x_3= 11/14 $$\n",
        "\n",
        "\n",
        "   $$ x_2 = -1/7 $$\n",
        "\n",
        " $$ x_1  = 1 - 2(-1/7) - 3(11/14)$$\n",
        "\n",
        "$$ x_1 = -15/14  $$\n",
        "  \n",
        "The point is $$ (-15/14 , -1/7, 11/14) $$\n",
        "\n",
        "Hessian,$$ H = \\begin{bmatrix} \n",
        "\td^2f/dx_2^2 &  d^2f/dx_2.dx_3  \\\\\n",
        "\td^2f/dx_2.dx_3 & d^2f/dx_3^2 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  = \n",
        "  \\begin{bmatrix} \n",
        "\t5 &  6  \\\\\n",
        "\t6 & 10 \\\\\n",
        "\t\\end{bmatrix} $$\n",
        "\n",
        "  Now the eigen values we have are : $$ \\lambda = 14, 1 $$\n",
        "\n",
        "  Since the values are positive, we can say that the given function is Convex in nature.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "xPyPQeBsFnG9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ySsUBtUNbtN"
      },
      "source": [
        ""
      ],
      "id": "1ySsUBtUNbtN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lUjYDOOUn8F",
        "outputId": "e2aee678-9603-4b77-92a4-164316a6fdc8"
      },
      "source": [
        "#For Gradient Descent\n",
        "import numpy as np\n",
        "\n",
        "def objective(x):\n",
        "    return (2 - (2*x[0]) - (3*x[1]))**2 + (x[0])**2 + (x[1] - 1)**2\n",
        "\n",
        "def grad(x):\n",
        "    return np.asarray([10*x[0] + 12*x[1] - 8, 12*x[0] + 20*x[1] - 14])\n",
        "\n",
        "def phi(a,x):\n",
        "    obj = objective(x)\n",
        "    corr = a*np.asarray([g**2 for g in grad(x)])\n",
        "    return obj - corr\n",
        "\n",
        "def line_search(x):\n",
        "    a = 1  # initialize step size\n",
        "    while np.linalg.norm(phi(a,x))<objective(x-a*grad(x)):\n",
        "        a = 0.5*a\n",
        "    return a\n",
        "\n",
        "eps = 1e-3\n",
        "x0 = [0,0]\n",
        "k = 0\n",
        "x = x0\n",
        "\n",
        "error = np.linalg.norm(grad(x))\n",
        "\n",
        "while error >= eps and k < 10000:  \n",
        "    a = line_search(x)\n",
        "    x = x - a*grad(x)\n",
        "    error = np.linalg.norm(grad(x))\n",
        "    k+=1\n",
        "    if k % 100 == 0: print(\"a:\", a, \"Error:\", error, \"x:\", x)\n",
        "\n",
        "x1 = 1 - 2*x[0] - 3*x[1]\n",
        "x = [x1, x[0], x[1]]\n",
        "print(\"Result:\", x)"
      ],
      "id": "4lUjYDOOUn8F",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: 0.125 Error: 0.9133611768834589 x: [-0.12476278  0.81285579]\n",
            "a: 0.0625 Error: 0.31115732430560283 x: [-0.13669289  0.79496066]\n",
            "a: 0.0625 Error: 0.3533428064728901 x: [-0.13585717  0.79621425]\n",
            "a: 0.0625 Error: 0.4012476298437458 x: [-0.13490814  0.79763779]\n",
            "a: 0.0625 Error: 0.4556472001293432 x: [-0.13383044  0.79925434]\n",
            "a: 0.0625 Error: 0.5174220494874027 x: [-0.13260664  0.80109004]\n",
            "a: 0.0625 Error: 0.5875720891508868 x: [-0.13121692  0.80317463]\n",
            "a: 0.0625 Error: 0.6672327943719045 x: [-0.12963878  0.80554183]\n",
            "a: 0.0625 Error: 0.7576935836566451 x: [-0.12784669  0.80822997]\n",
            "a: 0.125 Error: 0.8604186897840943 x: [-0.12581163  0.81128256]\n",
            "a: 0.125 Error: 0.9770708604353621 x: [-0.12350066  0.814749  ]\n",
            "a: 0.0625 Error: 0.33286148161825724 x: [-0.13626292  0.79560562]\n",
            "a: 0.0625 Error: 0.377989527786941 x: [-0.1353689   0.79694665]\n",
            "a: 0.0625 Error: 0.42923585637479506 x: [-0.13435367  0.79846949]\n",
            "a: 0.0625 Error: 0.4874299599687866 x: [-0.1332008   0.80019879]\n",
            "a: 0.0625 Error: 0.553513790487537 x: [-0.13189164  0.80216255]\n",
            "a: 0.0625 Error: 0.6285570059737116 x: [-0.13040498  0.80439254]\n",
            "a: 0.0625 Error: 0.7137742844865091 x: [-0.12871676  0.80692486]\n",
            "a: 0.125 Error: 0.8105449853430237 x: [-0.12679966  0.80980051]\n",
            "a: 0.125 Error: 0.9204354759534935 x: [-0.12462265  0.81306602]\n",
            "a: 0.0625 Error: 0.3135673456921772 x: [-0.13664515  0.79503228]\n",
            "a: 0.0625 Error: 0.35607956904885285 x: [-0.13580295  0.79629558]\n",
            "a: 0.0625 Error: 0.40435543189018225 x: [-0.13484657  0.79773014]\n",
            "a: 0.0625 Error: 0.45917634571349947 x: [-0.13376053  0.79935921]\n",
            "a: 0.0625 Error: 0.5214296627034495 x: [-0.13252725  0.80120913]\n",
            "a: 0.0625 Error: 0.5921230387522863 x: [-0.13112676  0.80330986]\n",
            "a: 0.0625 Error: 0.6724007437617341 x: [-0.1295364  0.8056954]\n",
            "a: 0.0625 Error: 0.7635621832314415 x: [-0.12773043  0.80840436]\n",
            "a: 0.125 Error: 0.867082930931118 x: [-0.1256796   0.81148059]\n",
            "a: 0.125 Error: 0.9846386130992996 x: [-0.12335074  0.81497389]\n",
            "a: 0.0625 Error: 0.3354396092303337 x: [-0.13621184  0.79568224]\n",
            "a: 0.0625 Error: 0.3809171877670901 x: [-0.1353109   0.79703365]\n",
            "a: 0.0625 Error: 0.43256043694216484 x: [-0.13428781  0.79856829]\n",
            "a: 0.0625 Error: 0.4912052740503005 x: [-0.13312601  0.80031098]\n",
            "a: 0.0625 Error: 0.5578009467544061 x: [-0.1318067   0.80228994]\n",
            "a: 0.0625 Error: 0.6334253979696475 x: [-0.13030853  0.80453721]\n",
            "a: 0.0625 Error: 0.7193027138579803 x: [-0.12860724  0.80708914]\n",
            "a: 0.125 Error: 0.816822937352852 x: [-0.12667529  0.80998706]\n",
            "a: 0.125 Error: 0.9275645679233553 x: [-0.12448142  0.81327787]\n",
            "a: 0.0625 Error: 0.3159960335302552 x: [-0.13659703  0.79510445]\n",
            "a: 0.0625 Error: 0.358837528800149 x: [-0.13574831  0.79637753]\n",
            "a: 0.0625 Error: 0.4074873049413422 x: [-0.13478453  0.79782321]\n",
            "a: 0.0625 Error: 0.46273282575421126 x: [-0.13369007  0.79946489]\n",
            "a: 0.0625 Error: 0.5254683162738183 x: [-0.13244724  0.80132915]\n",
            "a: 0.0625 Error: 0.5967092370367224 x: [-0.1310359   0.80344615]\n",
            "a: 0.0625 Error: 0.6776087207119191 x: [-0.12943323  0.80585016]\n",
            "a: 0.0625 Error: 0.7694762371452994 x: [-0.12761326  0.8085801 ]\n",
            "a: 0.125 Error: 0.8737987889370644 x: [-0.12554656  0.81168016]\n",
            "a: 0.125 Error: 0.992264980632168 x: [-0.12319966  0.81520051]\n",
            "a: 0.0625 Error: 0.3380377053348803 x: [-0.13616037  0.79575944]\n",
            "a: 0.0625 Error: 0.383867523489108 x: [-0.13525245  0.79712132]\n",
            "a: 0.0625 Error: 0.43591076753902086 x: [-0.13422144  0.79866785]\n",
            "a: 0.0625 Error: 0.49500982924868225 x: [-0.13305064  0.80042404]\n",
            "a: 0.0625 Error: 0.5621213085333284 x: [-0.13172111  0.80241833]\n",
            "a: 0.0625 Error: 0.6383314973499358 x: [-0.13021134  0.804683  ]\n",
            "a: 0.0625 Error: 0.7248739628322498 x: [-0.12849687  0.8072547 ]\n",
            "a: 0.125 Error: 0.8231495142782573 x: [-0.12654996  0.81017506]\n",
            "a: 0.125 Error: 0.9347488771828395 x: [-0.12433909  0.81349136]\n",
            "a: 0.0625 Error: 0.3184435323979114 x: [-0.13654855  0.79517718]\n",
            "a: 0.0625 Error: 0.36161684990617876 x: [-0.13569325  0.79646012]\n",
            "a: 0.0625 Error: 0.410643435435414 x: [-0.134722  0.797917]\n",
            "a: 0.0625 Error: 0.46631685196622563 x: [-0.13361907  0.7995714 ]\n",
            "a: 0.0625 Error: 0.5295382506166623 x: [-0.13236661  0.80145009]\n",
            "a: 0.0625 Error: 0.6013309570173312 x: [-0.13094434  0.80358349]\n",
            "a: 0.0625 Error: 0.6828570352496637 x: [-0.12932925  0.80600612]\n",
            "a: 0.125 Error: 0.7754360974576056 x: [-0.12749519  0.80875721]\n",
            "a: 0.125 Error: 0.8805666635922387 x: [-0.12541248  0.81188128]\n",
            "a: 0.125 Error: 0.9999504170262444 x: [-0.1230474   0.81542889]\n",
            "a: 0.0625 Error: 0.34065592459470656 x: [-0.1361085   0.79583725]\n",
            "a: 0.0625 Error: 0.38684071058438907 x: [-0.13519355  0.79720968]\n",
            "a: 0.0625 Error: 0.43928704760815596 x: [-0.13415455  0.79876818]\n",
            "a: 0.0625 Error: 0.4988438520464296 x: [-0.13297469  0.80053797]\n",
            "a: 0.0625 Error: 0.5664751330125164 x: [-0.13163486  0.80254771]\n",
            "a: 0.0625 Error: 0.643275596171309 x: [-0.13011339  0.80482992]\n",
            "a: 0.0625 Error: 0.7304883630618699 x: [-0.12838564  0.80742154]\n",
            "a: 0.125 Error: 0.8295250927359613 x: [-0.12642365  0.81036452]\n",
            "a: 0.125 Error: 0.9419888314091073 x: [-0.12419566  0.8137065 ]\n",
            "a: 0.0625 Error: 0.32090998799308146 x: [-0.13649968  0.79525047]\n",
            "a: 0.0625 Error: 0.3644176978180218 x: [-0.13563777  0.79654335]\n",
            "a: 0.0625 Error: 0.4138240112546796 x: [-0.13465899  0.79801151]\n",
            "a: 0.0625 Error: 0.4699286377041583 x: [-0.13354752  0.79967872]\n",
            "a: 0.0625 Error: 0.533639708012417 x: [-0.13228536  0.80157197]\n",
            "a: 0.0625 Error: 0.6059884738220176 x: [-0.13085207  0.80372189]\n",
            "a: 0.0625 Error: 0.6881459998036779 x: [-0.12922447  0.80616329]\n",
            "a: 0.125 Error: 0.7814421189550024 x: [-0.12737621  0.80893568]\n",
            "a: 0.125 Error: 0.8873869577837835 x: [-0.12527737  0.81208395]\n",
            "a: 0.125 Error: 1.0076953797906607 x: [-0.12289397  0.81565904]\n",
            "a: 0.0625 Error: 0.3432944228707599 x: [-0.13605623  0.79591565]\n",
            "a: 0.0625 Error: 0.38983692604491255 x: [-0.13513419  0.79729871]\n",
            "a: 0.0625 Error: 0.44268947813739096 x: [-0.13408714  0.79886928]\n",
            "a: 0.0625 Error: 0.5027075706804294 x: [-0.13289814  0.80065279]\n",
            "a: 0.0625 Error: 0.570862679372274 x: [-0.13154794  0.80267809]\n",
            "a: 0.0625 Error: 0.6482579887527617 x: [-0.13001468  0.80497797]\n",
            "a: 0.0625 Error: 0.7361462487684184 x: [-0.12827355  0.80758967]\n",
            "a: 0.125 Error: 0.8359500522599885 x: [-0.12629637  0.81055545]\n",
            "a: 0.125 Error: 0.9492848615918373 x: [-0.12405113  0.81392331]\n",
            "a: 0.0625 Error: 0.3233955471422497 x: [-0.13645044  0.79532433]\n",
            "a: 0.0625 Error: 0.36724023926832394 x: [-0.13558185  0.79662723]\n",
            "a: 0.0625 Error: 0.41702922173674484 x: [-0.13459549  0.79810676]\n",
            "a: 0.0625 Error: 0.47356839797524564 x: [-0.13347541  0.79978688]\n",
            "Result: [-1.1324098264763962, -0.13347541131132373, 0.7997868830330145]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1Ao4njWUrtx",
        "outputId": "41d2af1f-7aab-432e-cdb0-3f61c0fb6477"
      },
      "source": [
        "# NEWTONS\n",
        "import numpy as np\n",
        "\n",
        "def objective(x):\n",
        "    return (2 - (2*x[0]) - (3*x[1]))**2 + (x[0])**2 + (x[1] - 1)**2\n",
        "\n",
        "def grad(x):\n",
        "    return np.asarray([10*x[0] + 12*x[1] - 8, 12*x[0] + 20*x[1] - 14])\n",
        "\n",
        "def hessian():\n",
        "    return np.asarray([[10,12],[12,20]])\n",
        "\n",
        "def phi(a,x):\n",
        "    obj = objective(x)\n",
        "    corr = a*np.asarray([g**2 for g in grad(x)])\n",
        "    return obj - corr\n",
        "\n",
        "def line_search(x):\n",
        "    a = 1  # initialize step size\n",
        "    while np.linalg.norm(phi(a,x))<objective(x-a*grad(x)):  \n",
        "        a = 0.5*a\n",
        "    return a\n",
        "\n",
        "eps = 1e-3\n",
        "x0 = [0,0]\n",
        "k = 0\n",
        "x = np.asarray(x0).reshape(2,1)\n",
        "\n",
        "error = np.linalg.norm(grad(x))\n",
        "\n",
        "while error >= eps and k < 1000:  \n",
        "    a = line_search(x)\n",
        "    x = x - a*np.matmul(np.linalg.inv(hessian()), grad(x).reshape(2,1))\n",
        "    error = np.linalg.norm(grad(x))\n",
        "    k+=1\n",
        "    if k % 1 == 0: print(\"alpha:\", a, \"Error:\", error, \"x:\", x)\n",
        "\n",
        "x1 = 1 - 2*x[0] - 3*x[1]\n",
        "x = [x1, x[0], x[1]]\n",
        "print(\"result:\", x)\n",
        "\n"
      ],
      "id": "O1Ao4njWUrtx",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha: 0.0625 Error: 15.116733278059781 x: [[-0.00892857]\n",
            " [ 0.04910714]]\n",
            "alpha: 0.0625 Error: 14.171937448181044 x: [[-0.01729911]\n",
            " [ 0.09514509]]\n",
            "alpha: 0.0625 Error: 13.286191357669729 x: [[-0.02514648]\n",
            " [ 0.13830566]]\n",
            "alpha: 0.0625 Error: 12.45580439781537 x: [[-0.0325034]\n",
            " [ 0.1787687]]\n",
            "alpha: 0.0625 Error: 11.67731662295191 x: [[-0.03940051]\n",
            " [ 0.2167028 ]]\n",
            "alpha: 0.0625 Error: 10.947484334017416 x: [[-0.04586655]\n",
            " [ 0.25226602]]\n",
            "alpha: 0.0625 Error: 10.263266563141327 x: [[-0.05192846]\n",
            " [ 0.28560654]]\n",
            "alpha: 0.0625 Error: 9.621812402944995 x: [[-0.0576115 ]\n",
            " [ 0.31686327]]\n",
            "alpha: 0.0625 Error: 9.020449127760934 x: [[-0.06293936]\n",
            " [ 0.34616646]]\n",
            "alpha: 0.0625 Error: 8.456671057275875 x: [[-0.06793422]\n",
            " [ 0.3736382 ]]\n",
            "alpha: 0.0625 Error: 7.928129116196132 x: [[-0.0726169 ]\n",
            " [ 0.39939295]]\n",
            "alpha: 0.0625 Error: 7.432621046433875 x: [[-0.07700692]\n",
            " [ 0.42353804]]\n",
            "alpha: 0.0625 Error: 6.968082231031757 x: [[-0.08112255]\n",
            " [ 0.44617405]]\n",
            "alpha: 0.0625 Error: 6.532577091592273 x: [[-0.08498097]\n",
            " [ 0.46739532]]\n",
            "alpha: 0.0625 Error: 6.124291023367755 x: [[-0.08859823]\n",
            " [ 0.48729025]]\n",
            "alpha: 0.0625 Error: 5.741522834407272 x: [[-0.09198941]\n",
            " [ 0.50594175]]\n",
            "alpha: 0.0625 Error: 5.382677657256817 x: [[-0.09516864]\n",
            " [ 0.52342754]]\n",
            "alpha: 0.0625 Error: 5.046260303678267 x: [[-0.09814917]\n",
            " [ 0.53982046]]\n",
            "alpha: 0.0625 Error: 4.730869034698374 x: [[-0.10094342]\n",
            " [ 0.55518882]]\n",
            "alpha: 0.0625 Error: 4.435189720029727 x: [[-0.10356303]\n",
            " [ 0.56959667]]\n",
            "alpha: 0.0625 Error: 4.157990362527869 x: [[-0.10601891]\n",
            " [ 0.58310402]]\n",
            "alpha: 0.0625 Error: 3.898115964869877 x: [[-0.1083213 ]\n",
            " [ 0.59576716]]\n",
            "alpha: 0.0625 Error: 3.6544837170655087 x: [[-0.11047979]\n",
            " [ 0.60763885]]\n",
            "alpha: 0.0625 Error: 3.426078484748913 x: [[-0.11250338]\n",
            " [ 0.61876857]]\n",
            "alpha: 0.0625 Error: 3.211948579452107 x: [[-0.11440049]\n",
            " [ 0.62920268]]\n",
            "alpha: 0.0625 Error: 3.0112017932363506 x: [[-0.11617903]\n",
            " [ 0.63898465]]\n",
            "alpha: 0.0625 Error: 2.8230016811590795 x: [[-0.11784641]\n",
            " [ 0.64815525]]\n",
            "alpha: 0.0625 Error: 2.6465640760866354 x: [[-0.11940958]\n",
            " [ 0.65675269]]\n",
            "alpha: 0.0625 Error: 2.48115382133122 x: [[-0.12087505]\n",
            " [ 0.66481279]]\n",
            "alpha: 0.03125 Error: 2.4036177644146197 x: [[-0.12156199]\n",
            " [ 0.66859096]]\n",
            "alpha: 0.03125 Error: 2.3285047092766633 x: [[-0.12222747]\n",
            " [ 0.67225107]]\n",
            "alpha: 0.03125 Error: 2.255738937111767 x: [[-0.12287214]\n",
            " [ 0.67579679]]\n",
            "alpha: 0.03125 Error: 2.185247095327025 x: [[-0.12349668]\n",
            " [ 0.67923172]]\n",
            "alpha: 0.03125 Error: 2.1169581235980552 x: [[-0.12410169]\n",
            " [ 0.6825593 ]]\n",
            "alpha: 0.03125 Error: 2.0508031822356174 x: [[-0.1246878 ]\n",
            " [ 0.68578289]]\n",
            "alpha: 0.03125 Error: 1.9867155827907519 x: [[-0.12525559]\n",
            " [ 0.68890574]]\n",
            "alpha: 0.03125 Error: 1.9246307208285416 x: [[-0.12580564]\n",
            " [ 0.69193101]]\n",
            "alpha: 0.03125 Error: 1.8644860108026475 x: [[-0.1263385 ]\n",
            " [ 0.69486174]]\n",
            "alpha: 0.03125 Error: 1.806220822965065 x: [[-0.12685471]\n",
            " [ 0.69770088]]\n",
            "alpha: 0.03125 Error: 1.7497764222474055 x: [[-0.12735478]\n",
            " [ 0.7004513 ]]\n",
            "alpha: 0.03125 Error: 1.6950959090521738 x: [[-0.12783923]\n",
            " [ 0.70311577]]\n",
            "alpha: 0.03125 Error: 1.6421241618942928 x: [[-0.12830854]\n",
            " [ 0.70569697]]\n",
            "alpha: 0.03125 Error: 1.590807781835098 x: [[-0.12876318]\n",
            " [ 0.70819751]]\n",
            "alpha: 0.03125 Error: 1.5410950386527507 x: [[-0.12920362]\n",
            " [ 0.71061991]]\n",
            "alpha: 0.03125 Error: 1.4929358186948531 x: [[-0.12963029]\n",
            " [ 0.71296661]]\n",
            "alpha: 0.03125 Error: 1.4462815743606396 x: [[-0.13004363]\n",
            " [ 0.71523998]]\n",
            "alpha: 0.03125 Error: 1.401085275161868 x: [[-0.13044405]\n",
            " [ 0.7174423 ]]\n",
            "alpha: 0.03125 Error: 1.3573013603130624 x: [[-0.13083196]\n",
            " [ 0.7195758 ]]\n",
            "alpha: 0.03125 Error: 1.314885692803279 x: [[-0.13120775]\n",
            " [ 0.72164263]]\n",
            "alpha: 0.03125 Error: 1.2737955149031757 x: [[-0.13157179]\n",
            " [ 0.72364487]]\n",
            "alpha: 0.03125 Error: 1.2339894050624531 x: [[-0.13192446]\n",
            " [ 0.72558453]]\n",
            "alpha: 0.03125 Error: 1.1954272361542504 x: [[-0.13226611]\n",
            " [ 0.72746359]]\n",
            "alpha: 0.03125 Error: 1.1580701350244318 x: [[-0.13259708]\n",
            " [ 0.72928392]]\n",
            "alpha: 0.03125 Error: 1.1218804433049179 x: [[-0.1329177 ]\n",
            " [ 0.73104737]]\n",
            "alpha: 0.03125 Error: 1.0868216794516403 x: [[-0.13322831]\n",
            " [ 0.73275571]]\n",
            "alpha: 0.0625 Error: 1.0188953244859127 x: [[-0.13383011]\n",
            " [ 0.73606562]]\n",
            "alpha: 0.0625 Error: 0.9552143667055447 x: [[-0.1343943 ]\n",
            " [ 0.73916867]]\n",
            "alpha: 0.0625 Error: 0.8955134687864489 x: [[-0.13492323]\n",
            " [ 0.74207777]]\n",
            "alpha: 0.0625 Error: 0.8395438769872969 x: [[-0.1354191 ]\n",
            " [ 0.74480505]]\n",
            "alpha: 0.0625 Error: 0.7870723846755902 x: [[-0.13588398]\n",
            " [ 0.74736188]]\n",
            "alpha: 0.0625 Error: 0.7378803606333655 x: [[-0.1363198]\n",
            " [ 0.7497589]]\n",
            "alpha: 0.0625 Error: 0.6917628380937803 x: [[-0.13672838]\n",
            " [ 0.75200611]]\n",
            "alpha: 0.0625 Error: 0.6485276607129195 x: [[-0.13711143]\n",
            " [ 0.75411287]]\n",
            "alpha: 0.0625 Error: 0.6079946819183628 x: [[-0.13747054]\n",
            " [ 0.75608796]]\n",
            "alpha: 0.0625 Error: 0.5699950142984642 x: [[-0.1378072 ]\n",
            " [ 0.75793961]]\n",
            "alpha: 0.0625 Error: 0.5343703259048114 x: [[-0.13812282]\n",
            " [ 0.75967553]]\n",
            "alpha: 0.0625 Error: 0.5009721805357598 x: [[-0.13841872]\n",
            " [ 0.76130295]]\n",
            "alpha: 0.0625 Error: 0.46966141925227595 x: [[-0.13869612]\n",
            " [ 0.76282866]]\n",
            "alpha: 0.0625 Error: 0.44030758054900737 x: [[-0.13895618]\n",
            " [ 0.76425901]]\n",
            "alpha: 0.0625 Error: 0.41278835676469405 x: [[-0.13919999]\n",
            " [ 0.76559996]]\n",
            "alpha: 0.125 Error: 0.36118981216910695 x: [[-0.13965714]\n",
            " [ 0.76811425]]\n",
            "alpha: 0.125 Error: 0.3160410856479687 x: [[-0.14005714]\n",
            " [ 0.77031426]]\n",
            "alpha: 0.125 Error: 0.27653594994197145 x: [[-0.14040714]\n",
            " [ 0.77223926]]\n",
            "alpha: 0.125 Error: 0.24196895619922448 x: [[-0.14071339]\n",
            " [ 0.77392364]]\n",
            "alpha: 0.125 Error: 0.21172283667432054 x: [[-0.14098136]\n",
            " [ 0.77539747]]\n",
            "alpha: 0.125 Error: 0.18525748209002846 x: [[-0.14121583]\n",
            " [ 0.77668707]]\n",
            "alpha: 0.25 Error: 0.1389431115675214 x: [[-0.14162616]\n",
            " [ 0.77894388]]\n",
            "alpha: 0.25 Error: 0.10420733367564243 x: [[-0.14193391]\n",
            " [ 0.78063648]]\n",
            "alpha: 0.25 Error: 0.07815550025673298 x: [[-0.14216471]\n",
            " [ 0.78190593]]\n",
            "alpha: 0.5 Error: 0.0390777501283644 x: [[-0.14251093]\n",
            " [ 0.78381011]]\n",
            "alpha: 1 Error: 0.0 x: [[-0.14285714]\n",
            " [ 0.78571429]]\n",
            "result: [array([-1.07142857]), array([-0.14285714]), array([0.78571429])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcc-6MvNOzRO"
      },
      "source": [
        "###Problem 3:\n",
        "\n",
        " Given f(x) and g(x) are convex functions\n",
        "\n",
        " And asked if $ (af(x) + bg(x)) $ is also a convex function \n",
        "\n",
        " Let $x_1$ and $x_2$ be any points on the function f.\n",
        "\n",
        "  $$\\lambda x_1 + (1-\\lambda)x_2$$\n",
        "\n",
        " $$ f(\\lambda x_1 + (1 - \\lambda) x_2) <= \\lambda f(x_1) + (1 -\\lambda)f(x_2) $$\n",
        "\n",
        " $$ g(\\lambda x_1 + (1 - \\lambda) x_2) <= \\lambda g(x_1) + (1 -\\lambda)g(x_2) $$\n",
        "\n",
        " $\\implies$ $$af(\\lambda x_1 + (1 - \\lambda) x_2) + bg(\\lambda x_1 + (1 - \\lambda) x_2) $$\n",
        "\n",
        " $$ <= \\lambda (af(x_1) +g(x_1)) + (1 -\\lambda)(af(x_2) + bg(x_2)) $$\n",
        "\n",
        " So we can say that $ (af(x) + bg(x)) $ is a convex function.\n",
        "\n"
      ],
      "id": "xcc-6MvNOzRO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa9fealkUzy6"
      },
      "source": [
        "###Problem 4:\n",
        "\n",
        "$ f(x_1) >= f(x_0) + g_x^T (x_1 - x_0)  $\n",
        "\n",
        "For convex functions f(x) : x --> f and for $x_0, x_1 \\forall x$\n",
        "\n",
        "$$ f(x y + ( 1- \\lambda ) x <= \\lambda f(y) + (1-\\lambda) f(x)  $$\n",
        "$$ Here \\lambda \\in [0,1]  $$\n",
        "\n",
        "$$ f(x + \\lambda(y-x)) <= (1-\\lambda)f(x) + y  $$\n",
        "\n",
        "f is convex\n",
        "\n",
        "Using the first order Taylor Series expansion, we have\n",
        "\n",
        "$$ f(y) >= f(x) \\Delta f(x)^T(y-x)  $$\n",
        "\n",
        "$$\\Delta^2 f(x) >= 0 $$\n",
        "\n",
        "$$ f(y) >= f(x) + f'(x) (y-x)$$\n",
        "$$ w = \\lambda x + (1-\\lambda)y \\forall \\lambda \\in [0,1] $$\n",
        "$$  f(x) >= f(w) + f'(w) (x-w)    $$\n",
        "$$  f(y) >= f(w) + f'(w) (y-w)    $$"
      ],
      "id": "Wa9fealkUzy6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZk4hALMQ77j"
      },
      "source": [
        "### Problem 5:\n",
        "##a)\n",
        "To minimize the error between I and $a_k^T P$, we should adjust P.\n",
        "\n",
        "$$ min_p \\sum_k (a_k^T P_i - I )^2   $$\n",
        "\n",
        "Here, $ k= 1$ to  $ m ; 0<= P_i <= P_m $  $\\forall $   $i =1,2,..,m  $\n",
        "\n",
        "$$ f(P) = (a^T P - I)^2 $$\n",
        "$$ g = 2(a^T P - I ) a$$\n",
        "$$ H = 2 a a^T >= 0 $$\n",
        "\n",
        "##b)\n",
        "\n",
        "using Lemma,\n",
        "if $ d^THd >= 0 $ \n",
        "That means we have a positive semi definite Hessian\n",
        "\n",
        "$$\\implies d^THd = 2d^T[a][a]^T d $$\n",
        "$$ = 2u_k^2$$\n",
        "\n",
        "Here the Hessian is psd\n",
        "Thus the function we have is Convex\n",
        "\n",
        "##c)\n",
        "\n",
        "For unique solution, the problem should be convex that means the Hessian should be psd\n",
        "$$\\implies \\sum_k 2 d^T a_k a_k^T d = \\sum_k 2 u_k^2 > 0 $$\n",
        "\n",
        "Here the number of mirrors > number of lamps, which is the necessary condition for the solution.\n",
        "\n"
      ],
      "id": "aZk4hALMQ77j"
    }
  ]
}