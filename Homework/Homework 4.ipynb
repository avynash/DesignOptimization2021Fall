{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "Homework 4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_cPw-23PGrU"
      },
      "source": [
        "### Problem 1 (10 Points)\n",
        "\n",
        "Sketch graphically the problem \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{x_1,x_2} & \\quad f({\\bf x})=(x_1+1)^2+(x_2-2)^2\\\\\n",
        "{\\text{subject to }} & \\quad g_1 = x_1-2\\leq 0,{\\quad} g_3 = -x_1\\leq 0,\\\\\n",
        "& \\quad g_2 = x_2-1\\leq 0, {\\quad} g_4 = -x_2\\leq 0.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Find the optimum graphically. Determine directions of feasible descent at the corner points of the feasible domain. Show the gradient directions of $f$ and $g_i$s at these points. Verify graphical results analytically using the KKT conditions."
      ],
      "id": "7_cPw-23PGrU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIlVcDjnPHji"
      },
      "source": [
        "Solution:\n",
        "\n",
        "The location of the optimum is observed at (0,1) from the graph\n",
        "\n",
        "After applying the KKT Condition, we get\n",
        " $$ Δf(x) =[ 2(x_1 + 1); 2(x_2 - 2) ]$$\n",
        " $$ Δf(x, μ) at (0,1) = [(2 - \\mu_3); (-2+\\mu_2)] = [0;0]  $$ \n",
        " This whole term can be solved and we get a positive definite, that satisfies the condition\n",
        "\n"
      ],
      "id": "aIlVcDjnPHji"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egitz0NQPNll"
      },
      "source": [
        "### Problem 2 (10 Points)\n",
        "\n",
        "Graph the problem \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{x_1,x_2} & \\quad  f=-x_1\\\\\n",
        "{\\text{subject to }} & \\quad g_1=x_2-(1-x_1)^3\\leq 0{\\quad} {\\rm and}{\\quad} x_2\\geq 0.\n",
        "\\end{aligned}\n",
        "$$ \n",
        "\n",
        "Find the solution graphically. Then apply the optimality conditions. Can you find a solution based on the optimality conditions? Why? (From Kuhn and Tucker, 1951.)"
      ],
      "id": "egitz0NQPNll"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoTz58loXHw9"
      },
      "source": [
        "Solution\n",
        "\n",
        "The location of the optimum is observed at (1,0) from the graph\n",
        "After applying the KKT Condition, we get\n",
        "$$ Δf(x) =[ -1; 0 ]$$\n",
        " $$ Δf(x, μ) at (1,0) = [(-1); (\\mu_1+\\mu_2)] = [0;0]  $$ \n",
        "\n",
        " But after applying the KKT condition, it didnot satisfies the point.\n",
        " Therefore this is not the optimum that we want."
      ],
      "id": "RoTz58loXHw9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5OlCyFNPQfB"
      },
      "source": [
        "### Problem 3 (30 Points)\n",
        "\n",
        "Find a local solution to the problem \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\max_{x_1,x_2,x_3} & \\quad  f=x_1x_2+x_2x_3+x_1x_3\\\\\n",
        "{\\text{subject to }} & \\quad h=x_1+x_2+x_3-3=0.\n",
        "\\end{aligned}\n",
        "$$ \n",
        "\n",
        "Use two methods: reduced gradient and Lagrange multipliers."
      ],
      "id": "U5OlCyFNPQfB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEIBo99TYfuk"
      },
      "source": [
        "#Method 1\n",
        "\n",
        "Reduced Gradient\n",
        "\n",
        "If we take the first 2 points as $ d = [x_1 ; x_2] $\n",
        "\n",
        "And $s = x_3 = 6- x_1 - x_2$\n",
        "\n",
        "we have the gradient of these equations as\n",
        "\n",
        "$ Δ = [x_2 + x_3 ; x_1 + x_3] and [x_1 + x_2]       $\n",
        "\n",
        "If we equate them to 0, we get the optimum points at (1,1,1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Method 2\n",
        "\n",
        "Lagrange's Method\n",
        "\n",
        "$L =f(x) + λh(x) $\n",
        "  = $-x_1x_2-x_2x_3-x_1x_3 + λ(x_1 + x_2 + x_3 -3)  $\n",
        "\n",
        "The gradient is given by\n",
        "\n",
        "$Δ = [-x_2 - x_3 -λ ; -x_1 - x_3 -λ; -x_1 - x_2 -λ]$\n",
        "\n",
        "By solving these equations, we get the optimum point at (1,2,3)\n"
      ],
      "id": "NEIBo99TYfuk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOSP-_xpPHOx"
      },
      "source": [
        "### Problem 4 (20 Points)\n",
        "\n",
        "Use reduced gradient to\tfind the value(s) of the parameter $b$ for which the point $x_1=1$, $x_2=2$ is the solution to the problem \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\max_{x_1,x_2} & \\quad  f=2x_{1} + bx_2\\\\\n",
        "{\\text{subject to }} & \\quad g_1 = x_{1}^{2}+ x_{2}^{2}-5\\leq 0\\\\\n",
        "& \\quad g_2= x_1- x_2-2\\leq 0.\n",
        "\\end{aligned}\n",
        "$$ "
      ],
      "id": "SOSP-_xpPHOx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo0DJwgHPHXA"
      },
      "source": [
        "Solution\n",
        "\n",
        "If the optimum is located at (1,2), we can have\n",
        "$g_1 = 0$ and $g_2 = -3$\n",
        "\n",
        "Then, $ min f(x_1 - x_2)$ subjected to $g_1$ and $g_2$ \n",
        "\n",
        "Now, at (1,2) we can find the gradient and equate it to $x_1 = s$ and $x_2 = d$\n",
        "\n",
        "Now, we get b = 4\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "eo0DJwgHPHXA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "higher-bulgarian"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Problem 5 (30 Points)\n",
        "\n",
        "Find the solution for \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{x_1,x_2,x_3} & \\quad  f=x_{1}^{2}+x_{2}^{2}+x_{3}^{2}\\\\\n",
        "{\\text{subject to }} & \\quad h_1 = x_{1}^{2}/4+x_{2}^{2}/5+x_{3}^{2}/25-1=0\\\\\n",
        "& \\quad h_2 = x_1+x_2-x_3= 0,\n",
        "\\end{aligned}\n",
        "$$ \n",
        "\n",
        "by implementing the generalized reduced gradient algorithm."
      ],
      "id": "higher-bulgarian"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXc_kNNycQpU"
      },
      "source": [
        "Solution\n",
        "\n",
        "Here d = $x_1$ s = $[x_2; x_3] $\n"
      ],
      "id": "fXc_kNNycQpU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgxubOILch_C",
        "outputId": "47bf145f-7936-4af7-c23c-1f4b883c140b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "f1 = lambda x: x[0]**2 + x[1]**2 + x[2]**2\n",
        "pfd = lambda x: 2.0*x[0]\n",
        "pfs = lambda x: np.array([2.0*x[1], 2.0*x[2]])\n",
        "phs = lambda x: np.array([[2.0*x[1]/5.0, 2*x[2]/25.0],[1.0,-1.0]])\n",
        "phd = lambda x: np.array([[x[0]/2.0], [1.0]])\n",
        "dfd = lambda x: pfd(x) - np.matmul(np.matmul(pfs(x), np.linalg.inv(phs(x))), phd(x))\n",
        "err = 0.001\n",
        "k = 0\n",
        "def linesearch(x_, dfd_):\n",
        "    a = 1.0\n",
        "    x1 = np.array([0.0,0.0,0.0])\n",
        "    x1[0]= x_[0]-a*dfd_\n",
        "    x1[1:3]= x_[1:3] + a* np.transpose(np.matmul(np.matmul(np.linalg.inv(phs(x_)),phd(x_)),np.transpose([dfd(x_)]))) \n",
        "    while f1(x1) > (f1(x_)-a*0.3*dfd_**2):\n",
        "        a = 0.5*a\n",
        "        x1[0]= x_[0]-a*dfd_\n",
        "        x1[1:3]= x_[1:3] + a* np.transpose(np.matmul(np.matmul(np.linalg.inv(phs(x_)),phd(x_)),np.transpose([dfd(x_)])))\n",
        "    return a\n",
        "\n",
        "def LM(x):  \n",
        "    while np.linalg.norm(np.array([[x[0]**2/4.0+x[1]**2/5.0+x[2]**2/25.0-1.0], [x[0]+x[1]-x[2]]]))  > 0.001: \n",
        "        s =  np.transpose([x[1:3]]) - np.matmul(np.linalg.inv(phs(x)), np.array([[x[0]**2/4.0 + x[1]**2/5.0 + x[2]**2/25.0-1.0], [x[0]+x[1]-x[2]]])) \n",
        "        x=np.append(x[0], s)\n",
        "    return x\n",
        "\n",
        "x_k=np.array([0.0, 2.0412414523193148, 2.0412414523193148])\n",
        "\n",
        "while np.linalg.norm(dfd(x_k)) > err:\n",
        "    dfd_k = dfd(x_k)\n",
        "    a_k = linesearch(x_k, dfd_k)\n",
        "    d_k = x_k[0] - a_k * dfd_k\n",
        "    s_k = x_k[1:3] + a_k * np.transpose(np.matmul(np.matmul(np.linalg.inv(phs(x_k)), phd(x_k)),  np.transpose(dfd_k))) \n",
        "    x0 = np.append(d_k,s_k)\n",
        "    \n",
        "    x_k = LM(x0)\n",
        "    print('Iteration: '+str(k))\n",
        "    print('(x1,x2,x3)= '+str(x_k))\n",
        "    print('deflection = '+ str(np.linalg.norm(dfd(x_k))))\n",
        "    print('')\n",
        "    k += 1"
      ],
      "id": "TgxubOILch_C",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "(x1,x2,x3)= [-0.68041382  2.0160676   1.33565379]\n",
            "deflection = 3.0232956890507063\n",
            "\n",
            "Iteration: 1\n",
            "(x1,x2,x3)= [-1.43623774  1.55540686  0.11916912]\n",
            "deflection = 1.1226681674513554\n",
            "\n",
            "Iteration: 2\n",
            "(x1,x2,x3)= [-1.5064045   1.47078893 -0.03561557]\n",
            "deflection = 0.6226289462967629\n",
            "\n",
            "Iteration: 3\n",
            "(x1,x2,x3)= [-1.54531881  1.41836864 -0.12695017]\n",
            "deflection = 0.28416919823276343\n",
            "\n",
            "Iteration: 4\n",
            "(x1,x2,x3)= [-1.56307938  1.39341626 -0.16966313]\n",
            "deflection = 0.11315942045524086\n",
            "\n",
            "Iteration: 5\n",
            "(x1,x2,x3)= [-1.57015185  1.38307519 -0.18707666]\n",
            "deflection = 0.04085868626773115\n",
            "\n",
            "Iteration: 6\n",
            "(x1,x2,x3)= [-1.57270552  1.37927958 -0.19342594]\n",
            "deflection = 0.014109032181741643\n",
            "\n",
            "Iteration: 7\n",
            "(x1,x2,x3)= [-1.57358733  1.37796101 -0.19562632]\n",
            "deflection = 0.004789684951908235\n",
            "\n",
            "Iteration: 8\n",
            "(x1,x2,x3)= [-1.57388669  1.37751246 -0.19637423]\n",
            "deflection = 0.0016162434268895964\n",
            "\n",
            "Iteration: 9\n",
            "(x1,x2,x3)= [-1.5739877   1.37736099 -0.19662671]\n",
            "deflection = 0.000544269739602754\n",
            "\n"
          ]
        }
      ]
    }
  ]
}