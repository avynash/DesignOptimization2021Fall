{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Homework 3.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avynash/DesignOptimization2021Fall/blob/main/Homework_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "harmful-logging"
      },
      "source": [
        "### Problem 1 (50 points) \n",
        "\n",
        "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
        "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
        "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Here the saturation pressures are given by the Antoine equation\n",
        "\n",
        "$$\n",
        "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
        "$$\n",
        "\n",
        "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
        "system is given below.\n",
        "\n",
        "|             | $a_1$     | $a_2$      | $a_3$     |\n",
        "|:------------|:--------|:---------|:--------|\n",
        "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
        "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
        "\n",
        "\n",
        "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
        "\n",
        "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
        "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
        "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
        "\n",
        "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
        "\n",
        "1. Formulate the least square problem; \n",
        "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
        "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "id": "harmful-logging"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kktdfr_SSkjr"
      },
      "source": [
        "Problem 1:Solution"
      ],
      "id": "Kktdfr_SSkjr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Beq3YfbgMcHX"
      },
      "source": [
        ""
      ],
      "id": "Beq3YfbgMcHX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QbyzwKFSZ-v",
        "outputId": "ab426c69-2200-40e1-e5ca-3563c8e61c29"
      },
      "source": [
        "import numpy as np\n",
        "import torch as t\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x1_info = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
        "p_info = [28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
        "p_wsat = 10.0**(8.07131-1730.63/(20.0+233.426))\n",
        "p_dsat = 10.0**(7.43155-1554.679/(20.0+240.337))\n",
        "\n",
        "def loss(a):\n",
        "    total_loss = 0.0\n",
        "    for i in range(11):\n",
        "        x1 = x1_info[i]\n",
        "        p = p_info[i]\n",
        "        p_norm = x1*p_wsat*t.exp(a[0]*(a[1]*(1-x1)/(a[0]*x1+a[1]*(1-x1)))**2) + (1-x1)*p_dsat*t.exp(a[1]*(a[0]*x1/(a[0]*x1+a[1]*(1-x1)))**2)\n",
        "        total_loss = total_loss + (p_norm-p)**2\n",
        "    return total_loss\n",
        "\n",
        "error = 1\n",
        "A = Variable(t.tensor([1.0,1.0]), requires_grad = True)\n",
        "while error>= 0.05:\n",
        "    loss(A).backward()\n",
        "    error = t.linalg.norm(A.grad)\n",
        "    s = .2\n",
        "    while loss(A-s*A.grad) > loss(A):\n",
        "        s = .5*s\n",
        "    with t.no_grad():\n",
        "        A -= s*A.grad\n",
        "        A.grad.zero_()\n",
        "print(A)\n",
        "print(loss(A))\n"
      ],
      "id": "4QbyzwKFSZ-v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.9582, 1.6893], requires_grad=True)\n",
            "tensor(0.6702, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOGbs-AVfD-D",
        "outputId": "ace12c6d-d968-489a-8a5d-0977bb0006e0"
      },
      "source": [
        "from math import exp\n",
        "a = [1.9582,1.6893]\n",
        "for i in range(11):\n",
        "    x1 = x1_info[i]\n",
        "    p_norm = x1*p_wsat*exp(a[0]*(a[1]*(1-x1)/(a[0]*x1+a[1]*(1-x1)))**2) + (1-x1)*p_dsat*exp(a[1]*(a[0]*x1/(a[0]*x1+a[1]*(1-x1)))**2)\n",
        "    print((p_norm,3), p_info[i],((p_norm-p_info[i]/p_info[i],4)))"
      ],
      "id": "NOGbs-AVfD-D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28.824099527405245, 3) 28.1 (27.824099527405245, 4)\n",
            "(34.64328584864464, 3) 34.4 (33.64328584864464, 4)\n",
            "(36.452102883335144, 3) 36.7 (35.452102883335144, 4)\n",
            "(36.86661716636917, 3) 36.9 (35.86661716636917, 4)\n",
            "(36.87334015836798, 3) 36.8 (35.87334015836798, 4)\n",
            "(36.74916213399144, 3) 36.7 (35.74916213399144, 4)\n",
            "(36.38987768165168, 3) 36.5 (35.38987768165168, 4)\n",
            "(35.38456535962058, 3) 35.4 (34.38456535962058, 4)\n",
            "(32.94803191258182, 3) 32.9 (31.94803191258182, 4)\n",
            "(27.730647340243344, 3) 27.7 (26.730647340243344, 4)\n",
            "(17.47325208459706, 3) 17.5 (16.47325208459706, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSoPgn1PLiIh"
      },
      "source": [
        "### Problem 2 (50 points) \n",
        "\n",
        "Solve the following problem using Bayesian Optimization:\n",
        "$$\n",
        "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
        "$$\n",
        "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n"
      ],
      "id": "tSoPgn1PLiIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akGf_5ctHmSl"
      },
      "source": [
        "Problem 2: Solution"
      ],
      "id": "akGf_5ctHmSl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQnDmKjNHl22"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.gaussian_process as gp\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
        "\n",
        "    x_to_predict = x.reshape(-1, n_params)\n",
        "\n",
        "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
        "\n",
        "    if greater_is_better:\n",
        "        loss_optimum = np.max(evaluated_loss)\n",
        "    else:\n",
        "        loss_optimum = np.min(evaluated_loss)\n",
        "\n",
        "    scaling_factor = (-1) ** (not greater_is_better)\n",
        "\n",
        "    # In case sigma equals zero\n",
        "    with np.errstate(divide='ignore'):\n",
        "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
        "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
        "        expected_improvement[sigma == 0.0] == 0.0\n",
        "    return -1 * expected_improvement\n",
        "\n",
        "\n",
        "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
        "                               bounds=(0, 10), n_restarts=25):\n",
        "    best_x = None\n",
        "    best_acquisition_value = 1\n",
        "    n_params = bounds.shape[0]\n",
        "\n",
        "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
        "\n",
        "        res = minimize(fun=acquisition_func,\n",
        "                       x0=starting_point.reshape(1, -1),\n",
        "                       bounds=bounds,\n",
        "                       method='L-BFGS-B',\n",
        "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
        "    if res.fun < best_acquisition_value:\n",
        "            best_acquisition_value = res.fun\n",
        "            best_x = res.x\n",
        "\n",
        "    return best_x\n",
        "\n",
        "\n",
        "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
        "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    n_params = bounds.shape[0]\n",
        "\n",
        "    if x0 is None:\n",
        "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
        "            x_list.append(params)\n",
        "            y_list.append(sample_loss(params))\n",
        "    else:\n",
        "        for params in x0:\n",
        "            x_list.append(params)\n",
        "            y_list.append(sample_loss(params))\n",
        "\n",
        "    xp = np.array(x_list)\n",
        "    yp = np.array(y_list)\n",
        "\n",
        "    # Create the GP\n",
        "    if gp_params is not None:\n",
        "        model = gp.GaussianProcessRegressor(**gp_params)\n",
        "    else:\n",
        "        kernel = gp.kernels.Matern()\n",
        "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
        "                                            alpha=alpha,\n",
        "                                            n_restarts_optimizer=10,\n",
        "                                            normalize_y=True)\n",
        "\n",
        "    for n in range(n_iters):\n",
        "\n",
        "        model.fit(xp, yp)\n",
        "        if random_search:\n",
        "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
        "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
        "            next_sample = x_random[np.argmax(ei), :]\n",
        "        else:\n",
        "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
        "\n",
        "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
        "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
        "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
        "\n",
        "        # Sample loss for new set of parameters\n",
        "        cv_score = sample_loss(next_sample)\n",
        "\n",
        "        # Update lists\n",
        "        x_list.append(next_sample)\n",
        "        y_list.append(cv_score)\n",
        "\n",
        "        # Update xp and yp\n",
        "        xp = np.array(x_list)\n",
        "        yp = np.array(y_list)\n",
        "\n",
        "    return xp, yp"
      ],
      "id": "pQnDmKjNHl22",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E47-R56IBiB_",
        "outputId": "342f442c-8f98-4e3f-f063-aa8cba835b15"
      },
      "source": [
        "f = lambda x: (4.0-2.1*x[0]**2+x[0]**4/3.0)*x[0]**2+x[0]*x[1]+(-4+4*x[1]**2)*x[1]**2\n",
        "x, y = bayesian_optimisation(50, sample_loss=f, bounds=np.array([[-3, 3], [-2, 2]]), x0=None, n_pre_samples=10, gp_params=None, random_search=10, alpha=1e-5, epsilon=1e-7)\n",
        "print('The minimal value is ' + str(np.min(y)))\n",
        "print('The values of x1 and x2 are ' + str(x[np.where(y == np.amin(y))]))"
      ],
      "id": "E47-R56IBiB_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimal value is -0.46187413568427216\n",
            "The values of x1 and x2 are [[-0.14764154  0.94037585]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVx8-seOVdi"
      },
      "source": [
        ""
      ],
      "id": "XaVx8-seOVdi"
    }
  ]
}